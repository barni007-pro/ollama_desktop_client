# Ollama Desktop

---

> **Languages:** [English](README.md) | [Deutsch]

---

**Ollama Desktop** ist eine grafische Benutzeroberfl√§che (GUI) f√ºr **Ollama**. Sie erm√∂glicht es dir, lokal installierte KI-Modelle komfortabel zu steuern, Parameter fein abzustimmen und strukturierte JSON-Antworten zu erzwingen.

---

### ‚ú® Highlights
* **Vision-Unterst√ºtzung:** Bilder (.jpg,.png) hochladen und analysieren (z. B. mit llava) im Generate-Modus.
* **Dateien einlesen:** Importieren von Textdateien (.pdf, .txt, .json), um deren Inhalte direkt in den Kontext einzubinden.
* **RAG-Tool:** Chatten mit gro√üen Dokumenten (.pdf/.txt) durch lokale Wissensextraktion.
* **Chat-Modus:** Modellwechsel w√§hrend der Konversation m√∂glich.
* **Tools & Functions:** Lokale Python-Code-Ausf√ºhrung durch das Modell.
* **JSON-Modus:** Erzwingen von strukturierten Antworten.
* **Code-Ausf√ºhrung:** Direktes Ausf√ºhren von Python, PowerShell oder Batch-Skripten.
* **Screenshots:** Verabeiten von Screenshots durch ein Vision LLM.
* **Formelausgabe:** Anzeigen von mathematischen Formeln (LaTeX Unterst√ºtzung).

---

### üì• [Neueste Version herunterladen](https://github.com/barni007-pro/ollama_desktop_client/releases/latest)

Du hast die Wahl zwischen der **Portablen** Version (keine Installation erforderlich) und der **Setup**-Version (Standard-Installation).

## üì¶ Installation & Einrichtung

### üöÄ Option 1: Portable Version
1. **Download:** Lade die `Ollama_Desktop_Portable_x.x.x.x.zip` von der [Releases](https://github.com/barni007-pro/ollama_desktop_client/releases)-Seite herunter.
2. **Entpacken:** Entpacke das Archiv in einen Ordner deiner Wahl.
3. **Starten:** F√ºhre die `Ollama_Desktop.exe` direkt aus dem entpackten Ordner aus.

### üíª Option 2: Setup-Version (Installer)
1. **Download:** Lade die `Ollama_Desktop_Setup_x.x.x.x.zip` von der [Releases](https://github.com/barni007-pro/ollama_desktop_client/releases)-Seite herunter.
2. **Entpacken:** Entpacke die Installationsdatei aus dem ZIP-Archiv.
3. **Installieren:** Starte die Setup-Datei und folge den Anweisungen auf dem Bildschirm.
4. **Starten:** √ñffne die Anwendung nach der Installation √ºber das Startmen√º oder die Desktop-Verkn√ºpfung.

---

> [!IMPORTANT]
> **Ollama Server:** Stelle sicher, dass der Ollama-Server l√§uft (Terminal: `ollama serve`), bevor du den Desktop-Client startest.

---

## üñºÔ∏è UI-√úbersicht

<details>
  <summary><b>Hier klicken: Screenshots aller Tabs anzeigen (Bilder zum Vergr√∂√üern anklicken)</b></summary>

| Tab: Model Info | Tab: Model Parameter |
| :---: | :---: |
| [![Model Info](./screenshots/model_info.png)](./screenshots/model_info.png) | [![Model Parameter](./screenshots/model_parameter.png)](./screenshots/model_parameter.png) |
| **Tab: Tools** | **Tab: RAG Tool** |
| [![Tools](./screenshots/tools.png)](./screenshots/tools.png) | [![RAG Tool](./screenshots/rag_tool.png)](./screenshots/rag_tool.png) |
| **Tab: Request JSON** | **Tab: Response JSON** |
| [![Request JSON](./screenshots/request_json.png)](./screenshots/request_json.png) | [![Response JSON](./screenshots/response_json.png)](./screenshots/response_json.png) |
| **Tab: Response MarkDown** | **Tab: Response HTML** |
| [![Response MarkDown](./screenshots/response_markdown.png)](./screenshots/response_markdown.png) | [![Response HTML](./screenshots/response_html.png)](./screenshots/response_html.png) |
| **Tab: Code Parameter** | **Tab: Code Block** |
| [![Code Parameter](./screenshots/code_parameter.png)](./screenshots/code_parameter.png) | [![Code Block](./screenshots/code_block.png)](./screenshots/code_block.png) |

</details>

---

## üöÄ Schnellstart

1.  **Ollama-Server starten:** Starte den Ollama-Server im Hintergrund (Terminal: `ollama serve`).
2.  **LLM-Liste laden:** Klicke oben links in der App auf **Get LLM List**, um deine installierten Modelle zu laden.
3.  **Modell ausw√§hlen:** W√§hle ein Modell aus dem Dropdown-Men√º (z. B. `llama3` oder `gemma2`).
4.  **Prompt ausf√ºhren:** Gib deine Frage ein und dr√ºcke den **Play-Button (‚ñ∂)**.

*Tipp: Stelle sicher, dass die Adresse oben links korrekt ist (Standard: `127.0.0.1:11434`).*

---

## üîÑ Betriebsmodi & Vision-Unterst√ºtzung

√úber das **API**-Dropdown-Men√º kannst du steuern, wie die App mit dem Modell kommuniziert.

### 1. Generate-Modus (Einzelanfrage)
Dieser Modus ist f√ºr Einzelaufgaben (One-Shot-Tasks) konzipiert.
* **Vision / Bilder:** Dies ist der *erinzige* Modus, in dem du Bilder hochladen kannst (√ºber die Schaltfl√§chen `+ File` oder `+ Screenshot`). Nutze daf√ºr Vision-f√§hige Modelle wie *llava* oder *moondream*.
* **Verhalten:** Jede Anfrage steht f√ºr sich allein; das Modell "vergisst" vorherige Fragen sofort.
* **Kontext:** Du kannst jedoch Kontext-Token in die n√§chste Anfrage einbeziehen, um auch im Generate-Modus eine Konversation aufrechtzuerhalten.

### 2. Chat-Modus (Konversation)
Der gesamte Konversationsverlauf wird gespeichert und mit jeder neuen Nachricht gesendet.
* **Modellwechsel:** Du kannst das **LLM mitten in einer Konversation wechseln** (z. B. von einem schnellen 7B-Modell zu einem intelligenten 70B-Modell), ohne den roten Faden zu verlieren.

### üîÄ Die Br√ºcke: ‚ÄûGenerate > Chat‚Äú
Da der Chat-Modus keine Bilder direkt empfangen kann, bietet die App diesen Workflow an:
1. W√§hle **Generate** und lade ein Bild hoch (z. B. ‚ÄûBeschreibe dieses Bild‚Äú).
2. Warte auf die Antwort.
3. Klicke auf die Schaltfl√§che **Generate > Chat**.
*Die Analyse wird in den Chat-Verlauf kopiert, sodass du im Chat-Modus Folgefragen dazu stellen kannst.*

---

## üõ† Modell-Parameter

### 1. System-Prompt
Definiere die ‚ÄûPers√∂nlichkeit‚Äú der KI (z. B. ‚ÄûDu bist ein erfahrener C#-Entwickler‚Äú). Aktiviere das Kontrollk√§stchen **Use System Prompt**, um diese Anweisung vor jedem Chat zu senden.

### 2. Ausgabeformat (JSON-Modus)
Zwinge das Modell dazu, exakt in einem definierten **JSON-Schema**-Format zu antworten. Dies ist ideal f√ºr Entwickler, die strukturierte Daten ben√∂tigen.

### 3. Content-Prompt
Dieser Prompt erweitert die Eingabe um angeh√§ngte textbasierte Dateien wie **.txt**, **.json** oder **.pdf**.

### 4. Options-Parameter
| Parameter | Beschreibung |
| :--- | :--- |
| `temperature` | **Kreativit√§t.** `0.0` ist deterministisch; `0.7-0.8` ist nat√ºrlich (Standard); `1.2+` ist experimentell. |
| `top_p` | **Nucleus Sampling.** Ber√ºcksichtigt W√∂rter, die eine kumulative Wahrscheinlichkeit `P` erreichen. |
| `num_ctx` | **Kontextfenster.** Legt fest, wie viele Token das Modell gleichzeitig verarbeiten kann. `2048` ist Standard; `4096-8192` f√ºr Dokumente. |
| `repeat_penalty` | Verhindert, dass das Modell W√∂rter wiederholt (empfohlen: `1.1-1.2`). |
| `seed` | Ein fester Wert (z. B. `42`) stellt sicher, dass derselbe Prompt mit denselben Parametern immer die gleiche Antwort liefert. |

*Du kannst manuell benutzerdefinierte Parameter hinzuf√ºgen, indem du auf die leere Zeile (markiert mit `*`) in der Tabelle klickst.*

---

## üõ† Tools & Funktionsaufrufe (Function Calling)
Der Reiter **Tools** macht das LLM zu einem Agenten, der Aufgaben wie Wetterabfragen oder Berechnungen automatisch ausf√ºhren kann.
* **Tool JSON:** Definiere hier deine API-Schnittstelle, damit das Modell wei√ü, welche Parameter extrahiert werden m√ºssen.
* **Python-Code:** Hinterlege die Logik, die lokal ausgef√ºhrt werden soll. Die App f√ºhrt diesen Code automatisch aus, wenn das Modell das Tool anfordert.

---

## üìÑ RAG-Tool (Chat mit Dokumenten)
Lade **.txt**- oder **.pdf**-Dateien hoch, um sie als Wissensdatenbank zu nutzen.
* **Workflow:** Die App zerlegt die Datei in S√§tze, extrahiert Schl√ºsselw√∂rter/Synonyme aus deiner Anfrage und stellt dem LLM passende Textsegmente als Hintergrundwissen zur Verf√ºgung.
* **Delta-Parameter:** Steuert, wie viel Kontext (0-9 S√§tze) um einen Treffer herum an das Modell gesendet wird.

---

## üíª Code-Generierung & Ausf√ºhrung
F√ºhre Code in Python, PowerShell, Batch oder HTML/JavaScript direkt in der App aus.
* **Execute List:** Definiere deine Interpreter-Pfade (z. B. `python.exe`) im Reiter **Code Parameter**.
* **ShellExecute:** W√§hle zwischen der Code-Ausf√ºhrung im Hintergrund (Ausgabe wird erfasst) oder in einem externen Fenster.

---

## ‚öñÔ∏è Lizenzen & Drittanbieter-Komponenten
* **Ollama_Desktop, Newtonsoft.Json, Scintilla5.NET:** MIT-Lizenz.
* **WebView2:** Microsoft Corporation.
* **Markdig:** BSD-Clause 2.
* **PdfPig:** Apache-Lizenz 2.0.
* **Siticone.NetCore.UI:** Propriet√§re Lizenz.

---

## ‚òï Unterst√ºtzung
Ollama Desktop ist kostenlos und quelloffen. Wenn du die Entwicklung unterst√ºtzen m√∂chtest, kannst du via PayPal spenden:

**[√úber PayPal spenden](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=r.barnstorf@online.de&currency_code=EUR&source=url)**

Empf√§nger: `r.barnstorf@online.de`