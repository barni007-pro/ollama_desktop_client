# Ollama Desktop

---

> **Languages:** [English](README.md) | [Deutsch]

---

**Ollama Desktop** ist eine grafische Benutzeroberfl√§che (GUI) f√ºr **Ollama**. Sie erm√∂glicht es dir, lokal installierte KI-Modelle komfortabel zu steuern, Parameter fein abzustimmen und strukturierte JSON-Antworten zu erzwingen.

---

### ‚ú® Highlights
* **Vision-Unterst√ºtzung:** Bilder hochladen und analysieren (z. B. mit llava) im Generate-Modus.
* **RAG-Tool:** Chatten Sie mit Ihren Dokumenten (.pdf/.txt) durch lokale Wissensextraktion.
* **Chat-Modus:** Modellwechsel w√§hrend der Konversation m√∂glich.
* **Tools & Functions:** Lokale Python-Code-Ausf√ºhrung durch das Modell.
* **JSON-Modus:** Erzwingen von strukturierten Antworten.
* **Code-Ausf√ºhrung:** Direktes Ausf√ºhren von Python, PowerShell oder Batch-Skripten.

---

### üì• [Download Latest Version (ZIP)](https://github.com/barni007-pro/ollama_desktop_client/releases/latest)

## üì¶ Installation & Einrichtung

1. **Download:** Laden Sie die aktuelle `Ollama_Desktop_1.0.2.5.zip` von der [Releases](https://github.com/barni007-pro/ollama_desktop_client/releases)-Seite herunter.
2. **Entpacken:** Extrahieren Sie das Archiv in einen Ordner Ihrer Wahl.
3. **Ollama Server:** Stellen Sie sicher, dass der Ollama-Server l√§uft (Terminal: `ollama serve`).
4. **Starten:** F√ºhren Sie die `Ollama_Desktop.exe` im entpackten Ordner aus.
---

![Ollama Desktop Screenshot](Ollama_Desktop.png)

---

## üöÄ Schnellstart

1.  **Ollama-Server starten:** Starte den Ollama-Server im Hintergrund (Terminal: `ollama serve`).
2.  **LLM-Liste laden:** Klicke oben links in der App auf **Get LLM List**, um deine installierten Modelle zu laden.
3.  **Modell ausw√§hlen:** W√§hle ein Modell aus dem Dropdown-Men√º (z. B. `llama3` oder `gemma2`).
4.  **Prompt ausf√ºhren:** Gib deine Frage ein und dr√ºcke den **Play-Button (‚ñ∂)**.

*Tipp: Stelle sicher, dass die Adresse oben links korrekt ist (Standard: `127.0.0.1:11434`).*

---

## üîÑ Betriebsmodi & Vision-Unterst√ºtzung

√úber das **API**-Dropdown-Men√º kannst du steuern, wie die App mit dem Modell kommuniziert.

### 1. Generate-Modus (Einzelanfrage)
Dieser Modus ist f√ºr Einzelaufgaben (One-Shot-Tasks) konzipiert.
* **Vision / Bilder:** Dies ist der *erinzige* Modus, in dem du Bilder hochladen kannst (√ºber die Schaltfl√§chen `+ File` oder `+ Screenshot`). Nutze daf√ºr Vision-f√§hige Modelle wie *llava* oder *moondream*.
* **Verhalten:** Jede Anfrage steht f√ºr sich allein; das Modell "vergisst" vorherige Fragen sofort.
* **Kontext:** Du kannst jedoch Kontext-Token in die n√§chste Anfrage einbeziehen, um auch im Generate-Modus eine Konversation aufrechtzuerhalten.

### 2. Chat-Modus (Konversation)
Der gesamte Konversationsverlauf wird gespeichert und mit jeder neuen Nachricht gesendet.
* **Modellwechsel:** Du kannst das **LLM mitten in einer Konversation wechseln** (z. B. von einem schnellen 7B-Modell zu einem intelligenten 70B-Modell), ohne den roten Faden zu verlieren.

### üîÄ Die Br√ºcke: ‚ÄûGenerate > Chat‚Äú
Da der Chat-Modus keine Bilder direkt empfangen kann, bietet die App diesen Workflow an:
1. W√§hle **Generate** und lade ein Bild hoch (z. B. ‚ÄûBeschreibe dieses Bild‚Äú).
2. Warte auf die Antwort.
3. Klicke auf die Schaltfl√§che **Generate > Chat**.
*Die Analyse wird in den Chat-Verlauf kopiert, sodass du im Chat-Modus Folgefragen dazu stellen kannst.*

---

## üõ† Modell-Parameter

### 1. System-Prompt
Definiere die ‚ÄûPers√∂nlichkeit‚Äú der KI (z. B. ‚ÄûDu bist ein erfahrener C#-Entwickler‚Äú). Aktiviere das Kontrollk√§stchen **Use System Prompt**, um diese Anweisung vor jedem Chat zu senden.

### 2. Ausgabeformat (JSON-Modus)
Zwinge das Modell dazu, exakt in einem definierten **JSON-Schema**-Format zu antworten. Dies ist ideal f√ºr Entwickler, die strukturierte Daten ben√∂tigen.

### 3. Content-Prompt
Dieser Prompt erweitert die Eingabe um angeh√§ngte textbasierte Dateien wie **.txt**, **.json** oder **.pdf**.

### 4. Options-Parameter
| Parameter | Beschreibung |
| :--- | :--- |
| `temperature` | **Kreativit√§t.** `0.0` ist deterministisch; `0.7-0.8` ist nat√ºrlich (Standard); `1.2+` ist experimentell. |
| `top_p` | **Nucleus Sampling.** Ber√ºcksichtigt W√∂rter, die eine kumulative Wahrscheinlichkeit `P` erreichen. |
| `num_ctx` | **Kontextfenster.** Legt fest, wie viele Token das Modell gleichzeitig verarbeiten kann. `2048` ist Standard; `4096-8192` f√ºr Dokumente. |
| `repeat_penalty` | Verhindert, dass das Modell W√∂rter wiederholt (empfohlen: `1.1-1.2`). |
| `seed` | Ein fester Wert (z. B. `42`) stellt sicher, dass derselbe Prompt mit denselben Parametern immer die gleiche Antwort liefert. |

*Du kannst manuell benutzerdefinierte Parameter hinzuf√ºgen, indem du auf die leere Zeile (markiert mit `*`) in der Tabelle klickst.*

---

## üõ† Tools & Funktionsaufrufe (Function Calling)
Der Reiter **Tools** macht das LLM zu einem Agenten, der Aufgaben wie Wetterabfragen oder Berechnungen automatisch ausf√ºhren kann.
* **Tool JSON:** Definiere hier deine API-Schnittstelle, damit das Modell wei√ü, welche Parameter extrahiert werden m√ºssen.
* **Python-Code:** Hinterlege die Logik, die lokal ausgef√ºhrt werden soll. Die App f√ºhrt diesen Code automatisch aus, wenn das Modell das Tool anfordert.

---

## üìÑ RAG-Tool (Chat mit Dokumenten)
Lade **.txt**- oder **.pdf**-Dateien hoch, um sie als Wissensdatenbank zu nutzen.
* **Workflow:** Die App zerlegt die Datei in S√§tze, extrahiert Schl√ºsselw√∂rter/Synonyme aus deiner Anfrage und stellt dem LLM passende Textsegmente als Hintergrundwissen zur Verf√ºgung.
* **Delta-Parameter:** Steuert, wie viel Kontext (0-9 S√§tze) um einen Treffer herum an das Modell gesendet wird.

---

## üíª Code-Generierung & Ausf√ºhrung
F√ºhre Code in Python, PowerShell, Batch oder HTML/JavaScript direkt in der App aus.
* **Execute List:** Definiere deine Interpreter-Pfade (z. B. `python.exe`) im Reiter **Code Parameter**.
* **ShellExecute:** W√§hle zwischen der Code-Ausf√ºhrung im Hintergrund (Ausgabe wird erfasst) oder in einem externen Fenster.

---

## ‚öñÔ∏è Lizenzen & Drittanbieter-Komponenten
* **Ollama_Desktop, Newtonsoft.Json, Scintilla5.NET:** MIT-Lizenz.
* **WebView2:** Microsoft Corporation.
* **Markdig:** BSD-Clause 2.
* **PdfPig:** Apache-Lizenz 2.0.
* **Siticone.NetCore.UI:** Propriet√§re Lizenz.

---

## ‚òï Unterst√ºtzung
Ollama Desktop ist kostenlos und quelloffen. Wenn du die Entwicklung unterst√ºtzen m√∂chtest, kannst du via PayPal spenden:

**[√úber PayPal spenden](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=r.barnstorf@online.de&currency_code=EUR&source=url)**

Empf√§nger: `r.barnstorf@online.de`