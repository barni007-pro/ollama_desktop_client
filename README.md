# Ollama Desktop

**Ollama Desktop** ist eine grafische Benutzeroberfl√§che (GUI) f√ºr **Ollama**. Die Anwendung erm√∂glicht die komfortable Steuerung lokal installierter KI-Modelle, die Feinabstimmung von Parametern und die Erzwingung strukturierter JSON-Antworten.

---

## üöÄ Schnellstart

1. **Ollama-Server starten:** Starten Sie den Server im Hintergrund (Terminal: `ollama serve`).
2. **LLM-Liste laden:** Klicken Sie oben links auf **Get LLM List**.
3. **Modell w√§hlen:** W√§hlen Sie ein Modell aus dem Dropdown-Men√º (z. B. `llama3` oder `gemma2`).
4. **Anfrage stellen:** Geben Sie Ihre Frage ein und dr√ºcken Sie den **Play-Button (‚ñ∂)**.

*Hinweis: Stellen Sie sicher, dass die Adresse korrekt ist (Standard: `127.0.0.1:11434`).*

---

## ‚ú® Hauptfunktionen

### Betriebsmodi
* **Generate Modus:** Optimiert f√ºr Einzelanfragen ("One-Shot"). Dies ist der einzige Modus mit **Vision-Support** f√ºr Bilder (z. B. via *llava* oder *moondream*).
* **Chat Modus:** Speichert den gesamten Gespr√§chsverlauf. Erlaubt den **Modell-Wechsel** mitten in einer Unterhaltung.
* **Br√ºckenfunktion:** √úber den Button **Generate > Chat** k√∂nnen Bild-Analysen nahtlos in den Chat-Kontext √ºbernommen werden.

### RAG Tool (Chat mit Dokumenten)
Laden Sie eigene **.txt** oder **.pdf** Dateien hoch, um Fragen zu spezifischen Inhalten zu stellen.
* **Intelligente Suche:** Das System analysiert Anfragen und erstellt Listen von Suchw√∂rtern und Synonymen.
* **Delta-Parameter:** Steuern Sie den Kontextbereich um gefundene Textstellen (0-9 S√§tze).

### Tools & Function Calling
Verwandeln Sie das LLM in einen Agenten:
* **Automatisierung:** Definieren Sie Tool-Schnittstellen via JSON und hinterlegen Sie **Python-Code**, der bei Bedarf automatisch lokal ausgef√ºhrt wird.
* **Integration:** R√ºckgabewerte des Codes werden direkt in den Antwortprozess des Modells integriert.

### Code Generierung & Ausf√ºhrung
* **Interpreter-Support:** F√ºhren Sie generierten Code in Sprachen wie Python, PowerShell, Batch oder HTML/JavaScript direkt aus.
* **Konfiguration:** Hinterlegen Sie eigene Pfade zu Interpretern in der **Execute List**.

---

## üõ† Parameter-Steuerung

Passen Sie das Modellverhalten √ºber detaillierte Optionen an:

| Parameter | Beschreibung |
| :--- | :--- |
| `temperature` | Steuert Kreativit√§t (0.0 = deterministisch, 0.7+ = nat√ºrlich). |
| `num_ctx` | Legt die Gr√∂√üe des Kontext-Fensters fest (z. B. 4096 f√ºr Dokumente). |
| `repeat_penalty` | Bestraft Wortwiederholungen. |
| `JSON Mode` | Erzwingt die Antwort in einem definierten JSON-Schema. |
| `System Prompt` | Definiert die "Pers√∂nlichkeit" der KI. |

*Die Parameterliste kann manuell um eigene Eintr√§ge erweitert werden.*

---

## ‚öñÔ∏è Lizenzen & Komponenten

Dieses Projekt verwendet folgende Open-Source-Komponenten:

* **Ollama_Desktop (7soft):** MIT-Lizenz
* **Newtonsoft.Json:** MIT-Lizenz
* **Scintilla5.NET:** MIT-Lizenz
* **WebView2:** Microsoft Corporation
* **Markdig:** BSD-Clause 2
* **PdfPig:** Apache License 2.0
* **Siticone.NetCore.UI:** Propriet√§re Lizenz

---

## ‚òï Unterst√ºtzung

Die App ist komplett kostenlos und Open Source. Wenn Ihnen das Projekt gef√§llt, k√∂nnen Sie die Arbeit via PayPal unterst√ºtzen:

**[Jetzt via PayPal spenden](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=r.barnstorf@online.de&currency_code=EUR&source=url)**

Empf√§nger: `r.barnstorf@online.de`
